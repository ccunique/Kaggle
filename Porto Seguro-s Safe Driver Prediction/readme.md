# Tutorials
## 1.data exploration
- [The Essential Kickstarter](https://www.kaggle.com/asindico/porto-seguro-the-essential-kickstarter)
- [Steering Wheel of Fortune - Porto Seguro EDA：](https://www.kaggle.com/headsortails/steering-wheel-of-fortune-porto-seguro-eda) **教科书式可视化！！！给出了每个特征的详细分布、每个特征和结果的关系...供后续特征工程参考**

## 2.feature engineering
- [Reconstruction of 'ps_reg_03'](https://www.kaggle.com/pnagel/reconstruction-of-ps-reg-03)
- [Boruta feature elimination](https://www.kaggle.com/tilii7/boruta-feature-elimination)


## 3.model xgboost/lightgbm/catboost/rgf
- [xgb+lgb kfold LB 0.282](https://www.kaggle.com/rshally/porto-xgb-lgb-kfold-lb-0-282):average ensemble by cv with 2 models
- [Forza Baseline - Catboost Example](https://www.kaggle.com/the1owl/forza-baseline-catboost-example?scriptVersionId=1630224/code)
- [XGB classifier, upsampling LB 0.283](https://www.kaggle.com/ogrellier/xgb-classifier-upsampling-lb-0-283/code)
- [xgboost cv LB 0.284](https://www.kaggle.com/aharless/xgboost-cv-lb-284)
- [RGF + Target Encoding + Upsampling](https://www.kaggle.com/tunguz/rgf-target-encoding-0-282-on-lb)


## 4.model NN
- [Shallow Keras NN With Upsampling .27+ LB](https://www.kaggle.com/aquatic/shallow-keras-nn-with-upsampling-27-lb/code)
- [Simple Keras MLP](https://www.kaggle.com/akashdeepjassal/simple-keras-mlp/code)
- [Entity Embedding Neural Net](https://www.kaggle.com/aquatic/entity-embedding-neural-net/code)


## 5.model ensemble
- [Simple Stacker LB 0.284](https://www.kaggle.com/yekenot/simple-stacker-lb-0-284)
- [0.285(LB) - Average of (3XGBoost, LightGBM, NN)](https://www.kaggle.com/pluchme/0-285-lb-average-of-3xgboost-lightgbm-nn)


## 6.Some intresting discussion
- [训练数据和测试数据的分布是否相同，相信本地cv还是pulic LB？](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/43453),其中提到了一种名为[adversarial validation](http://fastml.com/adversarial-validation-part-one/)的判断训练数据与测试数据分布是否相似的办法
